{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "output_model_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLRhDkyYPia9UAcLh2APAB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/00SamYun/simple_chabot_model/blob/main/output_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdP5zEUH_SKu"
      },
      "source": [
        "# set runtime to TPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-ZhCt5UPK5k"
      },
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWxy-R_HNUJ7"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU0E9e1kMYc_"
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnJlt-oBM8ND"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import TFT5ForConditionalGeneration\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpIqIIQwNTxr"
      },
      "source": [
        "#### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_zLKjL-SN6-"
      },
      "source": [
        "train_dataset = tf.data.TFRecordDataset('gs://PATH_TO_BUCKET/output_model/train.tfrecord')\n",
        "valid_dataset = tf.data.TFRecordDataset('gs://PATH_TO_BUCKET/output_model/validation.tfrecord')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8IZjrr7e8ck"
      },
      "source": [
        "def read_tfrecord(example):\n",
        "    format = {\n",
        "        \"attention_mask\": tf.io.FixedLenFeature([], tf.string), \n",
        "        \"decoder_attention_mask\": tf.io.FixedLenFeature([], tf.string), \n",
        "        \"input_ids\": tf.io.FixedLenFeature([], tf.string), \n",
        "        \"labels\": tf.io.FixedLenFeature([], tf.string)\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example, format)\n",
        "\n",
        "    record = {k:tf.io.parse_tensor(v, tf.int32) for k,v in example.items()}\n",
        "    record = {k:tf.reshape(v, (100,)) for k, v in record.items()}\n",
        "\n",
        "    return record"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3311JPWfAU6"
      },
      "source": [
        "train_dataset = train_dataset.map(read_tfrecord)\n",
        "valid_dataset = valid_dataset.map(read_tfrecord)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwrZ9sPZNMBz"
      },
      "source": [
        "#### Create Strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yws0Kes1M8dF"
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yOrYHhINO_d"
      },
      "source": [
        "#### Setup Input Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xliWaM9TM9vA"
      },
      "source": [
        "BUFFER_SIZE = 1812 # total number of elements is 18120\n",
        "\n",
        "BATCH_SIZE_PER_REPLICA = 16\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "\n",
        "EPOCHS = 11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ftvW_gdN4K5"
      },
      "source": [
        "train_ds = train_dataset.shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\n",
        "valid_ds = valid_dataset.batch(GLOBAL_BATCH_SIZE)\n",
        "\n",
        "train_dist_ds = strategy.experimental_distribute_dataset(train_ds)\n",
        "valid_dist_ds = strategy.experimental_distribute_dataset(valid_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iff47PwGPFCy"
      },
      "source": [
        "#### Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8d_oYgqPDkW"
      },
      "source": [
        "def create_model():\n",
        "    model = TFT5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0BGrE3IPyAy"
      },
      "source": [
        "#### Define Metrics & Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV56md6hPwF9"
      },
      "source": [
        "with strategy.scope():\n",
        "\n",
        "    def compute_loss(per_example_loss):\n",
        "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
        "\n",
        "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "\n",
        "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXrn_DRCS-_F"
      },
      "source": [
        "#### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpijTWvTSdyh"
      },
      "source": [
        "with strategy.scope():\n",
        "    model = create_model()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcaXzyQgTFvU"
      },
      "source": [
        "def train_step(inputs):\n",
        "    x = inputs\n",
        "    y = x['labels']\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        outputs = model(x, training=True)\n",
        "        per_example_loss, logits = outputs[0], outputs[1]\n",
        "        loss = compute_loss(per_example_loss)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_accuracy.update_state(y, logits)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7OkBF3FTuUV"
      },
      "source": [
        "def test_step(inputs):\n",
        "    x = inputs\n",
        "    y = x['labels']\n",
        "\n",
        "    outputs = model(x, training=False)\n",
        "    t_loss, logits = outputs[0], outputs[1]\n",
        "\n",
        "    test_loss.update_state(t_loss)\n",
        "    test_accuracy.update_state(y, logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ5BwrUAUO8c"
      },
      "source": [
        "@tf.function\n",
        "def distributed_train_step(dataset_inputs):\n",
        "    per_replica_losses = strategy.run(train_step, args=(dataset_inputs, ))\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "\n",
        "@tf.function\n",
        "def distributed_test_step(dataset_inputs):\n",
        "    return strategy.run(test_step, args=(dataset_inputs,))\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for x in train_dist_ds:\n",
        "        total_loss += distributed_train_step(x)\n",
        "        num_batches += 1\n",
        "        train_loss = total_loss / num_batches\n",
        "    \n",
        "    for x in valid_dist_ds:\n",
        "        distributed_test_step(x)\n",
        "        \n",
        "    template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}\")\n",
        "    print(template.format(epoch+1, train_loss, train_accuracy.result()*100, test_loss.result(),\n",
        "                          test_accuracy.result()*100))\n",
        "    \n",
        "    test_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "# training on 11 epochs took approximately 30 minutes to run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKJRvo83XHz_"
      },
      "source": [
        "model.save_weights('gs://PATH_TO_BUCKET/output_model/saved_weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaZFhPMjAROL"
      },
      "source": [
        "# Note: model should be reloaded and tested on CPU"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
