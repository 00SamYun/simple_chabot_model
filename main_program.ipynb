{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_program.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNmmM4QExwXuFFmLNwlEusv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/00SamYun/simple_chabot_model/blob/main/main_program.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0FOw3e9AxTd"
      },
      "source": [
        "# runtime - CPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJA9J-otbvdk"
      },
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbi1wnmIbvNM"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFdFIiB-b263"
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llNXefaib4e3"
      },
      "source": [
        "import os\n",
        "import json\n",
        "from urllib.request import Request, urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "from itertools import repeat\n",
        "from random import choice\n",
        "\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "from official.nlp import bert\n",
        "import official.nlp.bert.configs\n",
        "import official.nlp.bert.bert_models\n",
        "import official.nlp.bert.tokenization\n",
        "\n",
        "from transformers import TFT5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfiDKUTwcJfy"
      },
      "source": [
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('vader_lexicon', quiet=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUyrBvskcOXy"
      },
      "source": [
        "#### Input Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhQV8AqgcOw7"
      },
      "source": [
        "class InputModel():\n",
        "    def __init__(self): \n",
        "\n",
        "        gs_folder = \"gs://cloud-tpu-checkpoints/bert/v3/uncased_L-12_H-768_A-12\"\n",
        "        config_file = os.path.join(gs_folder, \"bert_config.json\")\n",
        "        config_dict = json.loads(tf.io.gfile.GFile(config_file).read()) \n",
        "        config = bert.configs.BertConfig.from_dict(config_dict) \n",
        "\n",
        "        self.model = bert.bert_models.classifier_model(config, num_labels=3)[0]\n",
        "        self.model.load_weights('gs://PATH_TO_BUCKET/input_model/training_weights')\n",
        "\n",
        "        self.tokenizer = bert.tokenization.FullTokenizer(\n",
        "            vocab_file=os.path.join(gs_folder, 'vocab.txt'), do_lower_case=True)\n",
        "\n",
        "\n",
        "    def get_kws(self, user_input):\n",
        "\n",
        "        sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "        stopwords = sp.Defaults.stop_words\n",
        "        pos_list = ['CD', 'FW', 'PRP', 'WP', 'WRB']\n",
        "        pos_starters = ('JJ', 'NN', 'VB')\n",
        "\n",
        "        text = ' '.join([w for w in word_tokenize(user_input) if not w in stopwords])\n",
        "        tags = {tok.text: tok.tag_ for tok in sp(text)}\n",
        "\n",
        "        unigrams = [w for w,p in tags.items() if p in pos_list or p.startswith(pos_starters)]\n",
        "\n",
        "        return unigrams, user_input\n",
        "\n",
        "\n",
        "    def encode(self, text): \n",
        "\n",
        "        tokens = list(self.tokenizer.tokenize(text))\n",
        "        tokens.append('[SEP]')\n",
        "\n",
        "        return self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "    \n",
        "\n",
        "    def prepare_inputs(self, unigrams, user_input):\n",
        "\n",
        "        contexts = [user_input]*len(unigrams)\n",
        "\n",
        "        words = tf.constant([self.encode(w) for w in np.array(unigrams)])\n",
        "        contexts = tf.constant([self.encode(c) for c in np.array(contexts)])\n",
        "\n",
        "        cls = [self.tokenizer.convert_tokens_to_ids(['[CLS]'])]*words.shape[0]\n",
        "        input_word_ids = tf.concat([cls, words, contexts], axis=-1)\n",
        "\n",
        "        input_mask = tf.ones_like(input_word_ids)\n",
        "        type_cls = tf.zeros_like(cls)\n",
        "        type_words = tf.zeros_like(words)\n",
        "        type_contexts = tf.ones_like(contexts)\n",
        "        input_type_ids = tf.concat([type_cls, type_words, type_contexts], axis=-1)\n",
        "\n",
        "        inputs = {\n",
        "            'input_word_ids': input_word_ids,\n",
        "            'input_mask': input_mask,\n",
        "            'input_type_ids': input_type_ids}\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    \n",
        "    def assign(self, unigrams, model_inputs):\n",
        "\n",
        "        logits = self.model(model_inputs)\n",
        "        probabilities = tf.nn.softmax(logits)\n",
        "        scores = tf.math.argmax(probabilities, axis=-1)\n",
        "        pairs = zip(unigrams, scores)\n",
        "        dct = {'subject': [], 'predicate': [], 'object': []}\n",
        "\n",
        "        for kw, s in pairs: \n",
        "            if s == 0:\n",
        "                dct['subject'].append(kw)\n",
        "            elif s == 1:\n",
        "                dct['predicate'].append(kw)\n",
        "            elif s == 2:\n",
        "                dct['object'].append(kw)\n",
        "        \n",
        "        if dct['subject'] == []: dct['subject'] = ['You']\n",
        "        if dct['predicate'] == []: dct['predicate'] = [choice(unigrams)]\n",
        "        if dct['object'] == []: dct['object'] = [choice(unigrams)]\n",
        "\n",
        "        return dct\n",
        "\n",
        "    \n",
        "    def format_inp(self, tagged_text):\n",
        "\n",
        "        kws = list(tagged_text.values())\n",
        "        count = np.prod([len(w) for w in kws])\n",
        "        new_dct = {}\n",
        "\n",
        "        new_dct['subject'] = [w for w in kws[0] for _ in range(int(count/len(kws[0])))] \n",
        "        new_dct['predicate'] = [x for item in tagged_text['predicate'] for x in repeat(item, len(tagged_text['object']))]\n",
        "        new_dct['predicate'] *= int(count/len(new_dct['predicate']))\n",
        "        new_dct['object'] = tagged_text['object']*int(count/len(tagged_text['object']))\n",
        "\n",
        "        triples = zip(*new_dct.values())\n",
        "\n",
        "        transit_str = ' | '.join([' | '.join(triple) for triple in triples])\n",
        "\n",
        "        return transit_str\n",
        "\n",
        "\n",
        "    def extract_data(self, user_input):\n",
        "\n",
        "        kws = self.get_kws(user_input)\n",
        "\n",
        "        if kws[0] == []: return None\n",
        "        \n",
        "        model_inputs = self.prepare_inputs(*kws)\n",
        "        tagged_text = self.assign(kws[0], model_inputs)\n",
        "        transit_str = self.format_inp(tagged_text)\n",
        "\n",
        "        return transit_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_IOW8bQh8Hb"
      },
      "source": [
        "#### Output Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmRxIdNzoZ-C"
      },
      "source": [
        "class OutputModel():\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        weights_dir = 'gs://PATH_TO_BUCKET/output_model/saved_weights'\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam()\n",
        "        self.model = TFT5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "        self.model.load_weights(weights_path)\n",
        "\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "    \n",
        "    def encode(self, transit_str):\n",
        "\n",
        "        return self.tokenizer(transit_str, return_tensors='tf')['input_ids']\n",
        "\n",
        "    \n",
        "    def generate_text(self, transit_str):\n",
        "\n",
        "        if not transit_str: return \"I'm sorry. I don't understand what you're saying.\"\n",
        "\n",
        "        input_ids = self.encode(transit_str)\n",
        "        output_ids = self.model.generate(input_ids).numpy()[0]\n",
        "        sentence = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "        return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG8qDkv4wNSj"
      },
      "source": [
        "#### Greetings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99Gfl3rEwN-f"
      },
      "source": [
        "def pick_webpage(option):\n",
        "\n",
        "    if option == 'joke':\n",
        "        webpage_list = ([\"https://www.countryliving.com/life/a27452412/best-dad-jokes/\", \n",
        "                        \"https://parade.com/968634/parade/jokes-for-kids/\", \"https://parade.com/1041830/marynliles/clean-jokes/\"])\n",
        "    elif option == 'quote':\n",
        "        webpage_list = ([\"https://www.berries.com/blog/positive-quotes\", \n",
        "                        \"https://blog.hubspot.com/sales/18-motivational-quotes-to-start-your-day-list\", \"https://my.oberlo.com/blog/motivational-quotes\", \n",
        "                        \"https://www.shopify.my/blog/motivational-quotes\", \"https://wisdomquotes.com/quote-of-the-day/\"])\n",
        "    elif option == 'fun fact':\n",
        "        webpage_list = [\"https://www.scarymommy.com/interesting-facts/\", \"https://redtri.com/quirky-facts-and-trivia-for-kids/\"]\n",
        "\n",
        "\n",
        "    return random.choice(webpage_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjkqHgZzyYDJ"
      },
      "source": [
        "def get_webpage(url):\n",
        "\n",
        "    url = \"http://webcache.googleusercontent.com/search?q=cache:\" + url\n",
        "\n",
        "    page = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    page = urlopen(page).read()\n",
        "    \n",
        "    return page"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ5aYaw2yZov"
      },
      "source": [
        "def get_jokes(html_doc, url):\n",
        "\n",
        "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "\n",
        "    step = [x for x in soup.find_all('li') if not x.a]\n",
        "    text_list = [x.get_text().replace('\\\"', '') for x in step]\n",
        "\n",
        "    if text_list: return text_list\n",
        "    \n",
        "    data = json.loads(soup.find('script', type='application/ld+json').text)['articleBody']\n",
        "\n",
        "    text_list = data.split('[NUM]')[1:-1]\n",
        "\n",
        "    if text_list: return text_list\n",
        "\n",
        "    print(url)\n",
        "\n",
        "\n",
        "def get_quotes(html_doc, url): \n",
        "\n",
        "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "\n",
        "    quote = soup.find_all('p', attrs={'class': 'box-quote'}) # works for index 0 only\n",
        "    name = soup.find_all('p', attrs={'class': 'box-name'})\n",
        "    step = zip([x.get_text() for x in quote], [x.get_text() for x in name])\n",
        "    text_list = ['.'.join(x) for x in step]\n",
        "\n",
        "    if text_list: return text_list\n",
        "\n",
        "    step = [x.get_text() for x in soup.find_all('p')] # works for index 1 only\n",
        "    text_list = [x for x in step if x and x[0].isdigit()]\n",
        "\n",
        "    if text_list: return text_list\n",
        "\n",
        "    step = [x.get_text() for x in soup.find_all('li') if not x.a] # works for index 2 and 3\n",
        "    text_list = [x for x in step if len(x) >= 40]\n",
        "\n",
        "    if text_list: return text_list\n",
        "\n",
        "    text_list = [x.get_text() for x in soup.find_all('blockquote')] # works for index 4 only \n",
        "\n",
        "    if text_list: return text_list\n",
        "\n",
        "    print(url)\n",
        "\n",
        "\n",
        "def get_facts(html_doc, url):\n",
        "\n",
        "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "\n",
        "    try:\n",
        "        step = soup.find('div', {'class': 'entry-content'}) # only works for index 0\n",
        "        step2 = list(map(lambda x: re.findall(r'^\\d+\\.(.+)', x), [x.get_text() for x in step.find_all('p')]))\n",
        "        text_list = [x[0].strip() for x in step2 if x]\n",
        "\n",
        "        if text_list: return text_list\n",
        "\n",
        "    except:\n",
        "        step = [x.get_text() for x in soup.find_all('p')] # only works for index 1\n",
        "        step2 = [re.findall(r'^\\d+\\.(.+)', x) for x in step]\n",
        "\n",
        "        text_list = [x[0].strip() for x in step2 if x]\n",
        "\n",
        "        if text_list: return text_list\n",
        "\n",
        "        print(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZBr-yxcyasm"
      },
      "source": [
        "def output_joke(jokes_list):\n",
        "\n",
        "    joke = random.choice(jokes_list)\n",
        "\n",
        "    if '?' in joke:\n",
        "        question = joke[:joke.index('?')+1].strip()\n",
        "        answer = joke[joke.index('?')+1:].strip()\n",
        "\n",
        "        print(question)\n",
        "        input()\n",
        "        print(answer)\n",
        "    \n",
        "    else:\n",
        "        print(joke.strip())\n",
        "\n",
        "\n",
        "def output_quote(quotes_list):\n",
        "\n",
        "    text = random.choice(quotes_list)\n",
        "\n",
        "    person = re.findall(r'[\\w\\s]+', text)[-1]\n",
        "\n",
        "    quote_step = re.sub(person, '', text)\n",
        "    quote = re.sub(r'^[^(a-zA-Z)]*|[^(a-zA-Z.)]*$', '', quote_step)\n",
        "\n",
        "    if person.lower() in ['unknown', 'anonymous', ' ']: \n",
        "        print('Someone once said {}'.format(quote))\n",
        "    \n",
        "    else:\n",
        "        print('{} once said {}'.format(person, quote))\n",
        "\n",
        "    print('What do you think?')\n",
        "\n",
        "\n",
        "def output_fact(facts_list):\n",
        "\n",
        "    openers = ['Did you know? ', 'Apparently ', 'Fun fact! ']\n",
        "\n",
        "    opener = random.choice(openers)\n",
        "\n",
        "    fact = random.choice(facts_list)\n",
        "\n",
        "    print(opener + fact)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mc30PIHybtz"
      },
      "source": [
        "def greet_by_joke():\n",
        "\n",
        "    url = pick_webpage('joke')\n",
        "    page = get_webpage(url)\n",
        "    jokes_list = get_jokes(page, url)\n",
        "    output_joke(jokes_list)\n",
        "\n",
        "\n",
        "def greet_by_quote():\n",
        "\n",
        "    url = pick_webpage('quote')\n",
        "    page = get_webpage(url)\n",
        "    quotes_list = get_quotes(page, url)\n",
        "    output_quote(quotes_list)\n",
        "\n",
        "\n",
        "def greet_by_fact():\n",
        "\n",
        "    url = pick_webpage('fun fact')\n",
        "    page = get_webpage(url)\n",
        "    facts_list = get_facts(page, url)\n",
        "    output_fact(facts_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG_ot7kzyctc"
      },
      "source": [
        "def greetings():\n",
        "\n",
        "    basic_greeting = ['Hi!', 'Hey!', 'Hello!'] \n",
        "\n",
        "    input()\n",
        "    print(random.choice(basic_greeting))\n",
        "\n",
        "    option = random.randint(0,2)\n",
        "\n",
        "    if option == 0:\n",
        "        greet_by_joke()\n",
        "    elif option == 1:\n",
        "        greet_by_quote()\n",
        "    elif option == 2:\n",
        "        greet_by_fact() \n",
        "    \n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    sentiment = sid.polarity_scores(input())['compound']\n",
        "\n",
        "    if sentiment >= 0.05:\n",
        "        print('\\U0001F604')\n",
        "    elif sentiment > -0.05 and sentiment < 0.05:\n",
        "        print('\\U0001F610')\n",
        "    elif sentiment <= -0.05:\n",
        "        print('\\U0001F61E')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTelSZ7ZyfL2"
      },
      "source": [
        "#### Main Program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqm3bI2R0b8c"
      },
      "source": [
        "input_model = InputModel()\n",
        "output_model = OutputModel()\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItT7zDDgydxf"
      },
      "source": [
        "print('To end the conversation at any point, type \"bye\".')\n",
        "print('To start the conversation, please greet the bot!')\n",
        "\n",
        "greetings()\n",
        "\n",
        "while True:\n",
        "    \n",
        "    user_input = str(input())\n",
        "\n",
        "    if user_input.lower() == 'bye':\n",
        "        print('Bye!')\n",
        "        break\n",
        "    \n",
        "    transit_data = input_model.extract_data(user_input)\n",
        "\n",
        "    output_text = output_model.generate_text(transit_data)\n",
        "\n",
        "    print(output_text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}