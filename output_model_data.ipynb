{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "output_model_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOEJbDhVWNu2zZmo53EtLUF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/00SamYun/simple_chabot_model/blob/main/output_model_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhWiN022IULf"
      },
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q8-Y7vrTWF2"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA8k076CIQKc"
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzO-_CCmIVLo"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from transformers import T5Tokenizer\n",
        "\n",
        "tf.get_logger().setLevel('WARNING')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZNNSZcCHG-s"
      },
      "source": [
        "#### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zezp5lQqIfNl"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSR5xbHvIopx"
      },
      "source": [
        "dataset, info = tfds.load('web_nlg', shuffle_files=True, with_info=True)\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVG-e5RKIibb"
      },
      "source": [
        "train_dataset = dataset['train']\n",
        "valid_dataset = dataset['validation']\n",
        "test_dataset = dataset['test_all']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMfAEPC7IkkA"
      },
      "source": [
        "train_dataset = train_dataset.take(-1)\n",
        "valid_dataset = valid_dataset.take(-1)\n",
        "test_dataset = test_dataset.take(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkbpU4mEImzd"
      },
      "source": [
        "train_dataset = train_dataset.map(lambda x: (x['input_text']['table']['content'], x['target_text']))\n",
        "valid_dataset = valid_dataset.map(lambda x: (x['input_text']['table']['content'], x['target_text']))\n",
        "test_dataset = test_dataset.map(lambda x: (x['input_text']['table']['content'], x['target_text']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSDMZ26kIl-u"
      },
      "source": [
        "def encode(example, encoder_max_len=100, decoder_max_len=100):\n",
        "\n",
        "    triples = example[0].numpy().reshape((-1,3)).tolist()\n",
        "    triples = b' | '.join([b' ; '.join(kw) for kw in triples]).decode()\n",
        "    sentence = example[1].numpy().decode() \n",
        "\n",
        "    triples_plus = f'webNLG: {str(triples)} </s>'\n",
        "    sentence_plus = f'{sentence} </s>'\n",
        "\n",
        "    encoder_inputs = tokenizer(triples_plus, pad_to_max_length=True, max_length=encoder_max_len, return_tensors='tf')\n",
        "    decoder_inputs = tokenizer(sentence_plus, pad_to_max_length=True, max_length=decoder_max_len, return_tensors='tf')\n",
        "\n",
        "    input_ids = encoder_inputs['input_ids'][0].numpy()\n",
        "    input_attention = encoder_inputs['attention_mask'][0].numpy()\n",
        "    target_ids = decoder_inputs['input_ids'][0].numpy()\n",
        "    target_attention = decoder_inputs['attention_mask'][0].numpy()\n",
        "\n",
        "    outputs = {'input_ids':input_ids, 'attention_mask': input_attention, \n",
        "               'labels':target_ids, 'decoder_attention_mask':target_attention}\n",
        "    \n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdDf_565JXW1"
      },
      "source": [
        "train_data = []\n",
        "\n",
        "for elem in train_dataset:\n",
        "    train_data.append(encode(elem))\n",
        "\n",
        "valid_data = []\n",
        "\n",
        "for elem in valid_dataset:\n",
        "    valid_data.append(encode(elem))\n",
        "\n",
        "test_data = []\n",
        "\n",
        "for elem in test_dataset:\n",
        "    test_data.append(encode(elem))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m2bnqe0KFBh"
      },
      "source": [
        "def _bytes_feature(value):\n",
        "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "    if isinstance(value, type(tf.constant(0))):\n",
        "        value = value.numpy() \n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI4d8zM2J6Vj"
      },
      "source": [
        "def serialize_example(element):\n",
        "    input_ids, att_mask, labels, dec_att_mask = [tf.io.serialize_tensor(x) for x in element.values()]\n",
        "\n",
        "    feature = {\n",
        "        'input_ids': _bytes_feature(input_ids),\n",
        "        'attention_mask': _bytes_feature(att_mask),\n",
        "        'labels': _bytes_feature(labels),\n",
        "        'decoder_attention_mask': _bytes_feature(dec_att_mask)\n",
        "        }\n",
        "\n",
        "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "    return example.SerializeToString()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yElv3LljM_06"
      },
      "source": [
        "with tf.io.TFRecordWriter('gs://PATH_TO_BUCKET/output_model/train.tfrecord') as writer:\n",
        "    for elem in train_data:\n",
        "        example = serialize_example(elem)\n",
        "        writer.write(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fso_G8FmOG_0"
      },
      "source": [
        "with tf.io.TFRecordWriter('gs://PATH_TO_BUCKET/output_model/validation.tfrecord') as writer:\n",
        "    for elem in valid_data:\n",
        "        example = serialize_example(elem)\n",
        "        writer.write(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Mhmr4QTQ0K"
      },
      "source": [
        "with tf.io.TFRecordWriter('gs://PATH_TO_BUCKET/output_model/test.tfrecord') as writer:\n",
        "    for elem in test_data:\n",
        "        example = serialize_example(elem)\n",
        "        writer.write(example)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}