{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "input_model_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcW7dJcBKm0dTuls3m3p9u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/00SamYun/simple_chabot_model/blob/main/input_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrpswV5a75Hx"
      },
      "source": [
        "# set runtime to TPU "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs7_nP7P2mrk"
      },
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1Zv9t7DQ8vl"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRmBhDCs2qfe"
      },
      "source": [
        "import os\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from official.modeling import tf_utils \n",
        "from official import nlp \n",
        "from official.nlp import bert\n",
        "\n",
        "import official.nlp.bert.configs\n",
        "import official.nlp.bert.bert_models\n",
        "import official.nlp.bert.tokenization\n",
        "import official.nlp.optimization\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q5eYevf2yzG"
      },
      "source": [
        "#### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVWbN8S22yKT"
      },
      "source": [
        "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/v3/uncased_L-12_H-768_A-12\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuvWTSd63LAk"
      },
      "source": [
        "def extract_info(element):\n",
        "    context = element['target_text'].numpy().decode()\n",
        "    words = element['input_text']['table']['content'].numpy()\n",
        "\n",
        "    mapping = list(set([(w.decode(), i%3) for i, w in enumerate(words)]))\n",
        "    \n",
        "    mapping = [(t[0], [int(n==t[1]) for n in range(3)]) for t in mapping]\n",
        "    \n",
        "    return [t[0] for t in mapping], [context]*len(mapping), [t[1] for t in mapping]\n",
        "\n",
        "\n",
        "def create_data(ds):\n",
        "    dataset = {'word':[], 'context':[], 'label':[]}\n",
        "\n",
        "    for elem in ds:\n",
        "        w,c,l = extract_info(elem)\n",
        "        dataset['word'] += w\n",
        "        dataset['context'] += c\n",
        "        dataset['label'] += l\n",
        "        \n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUqKP6FU24iS"
      },
      "source": [
        "with tf.device('/job:localhost'):\n",
        "    dart, info = tfds.load('dart', with_info=True, shuffle_files=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu0uZfv23Ox9"
      },
      "source": [
        "with tf.device('/job:localhost'):\n",
        "    \n",
        "    train_dataset = create_data(dart['train'].take(-1))\n",
        "    valid_dataset = create_data(dart['validation'].take(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmDGUZfh4Mk5"
      },
      "source": [
        "tokenizer = bert.tokenization.FullTokenizer(vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"), do_lower_case=True)\n",
        "\n",
        "def encode(inp, tokenizer):\n",
        "    tokens = list(tokenizer.tokenize(inp))\n",
        "    tokens.append('[SEP]')\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "\n",
        "def prepare_inputs(ds, tokenizer):\n",
        "\n",
        "    words = tf.ragged.constant([encode(w, tokenizer) for w in np.array(ds['word'])])\n",
        "    contexts = tf.ragged.constant([encode(c, tokenizer) for c in np.array(ds['context'])])\n",
        "\n",
        "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*words.shape[0]\n",
        "    input_word_ids = tf.concat([cls, words, contexts], axis=-1)\n",
        "\n",
        "    input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
        "    type_cls = tf.zeros_like(cls)\n",
        "    type_words = tf.zeros_like(words)\n",
        "    type_contexts = tf.ones_like(contexts)\n",
        "    input_type_ids = tf.concat([type_cls, type_words, type_contexts], axis=-1).to_tensor()\n",
        "\n",
        "    inputs = {\n",
        "        'input_word_ids': input_word_ids.to_tensor(),\n",
        "        'input_mask': input_mask,\n",
        "        'input_type_ids': input_type_ids}\n",
        "\n",
        "    return inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C81hYBb15bhy"
      },
      "source": [
        "train_ds = prepare_inputs(train_dataset, tokenizer)\n",
        "valid_ds = prepare_inputs(valid_dataset, tokenizer)\n",
        "\n",
        "train_label_ds = tf.convert_to_tensor(train_dataset['label'])\n",
        "valid_label_ds = tf.convert_to_tensor(valid_dataset['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXxn7GIb7sv0"
      },
      "source": [
        "#### Create Strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsgKdF727sVo"
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUkNiC7w5yz7"
      },
      "source": [
        "#### Setup Input Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j19IFPk_5o2K"
      },
      "source": [
        "BATCH_SIZE_PER_REPLICA = 32\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "EPOCHS = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8Hhb7Z76Mhb"
      },
      "source": [
        "#### Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vjutvne6BUA"
      },
      "source": [
        "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/v3/uncased_L-12_H-768_A-12\"\n",
        "\n",
        "hub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\"\n",
        "\n",
        "bert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\n",
        "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read()) \n",
        "\n",
        "bert_config = bert.configs.BertConfig.from_dict(config_dict) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPOY8emd6bE5"
      },
      "source": [
        "def create_model():\n",
        "    bert_classifier, bert_encoder = bert.bert_models.classifier_model(bert_config, num_labels=3)\n",
        "\n",
        "    return bert_classifier, bert_encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CxqnPXQ8fHN"
      },
      "source": [
        "with strategy.scope():\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    metrics = tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)\n",
        "    optimizer = tf.keras.optimizers.SGD()\n",
        "\n",
        "    bert_classifier, bert_encoder = create_model()\n",
        "\n",
        "    checkpoint = tf.train.Checkpoint(encoder=bert_encoder)\n",
        "    checkpoint.read(os.path.join(gs_folder_bert, 'bert_model.ckpt')).assert_consumed() \n",
        "\n",
        "    bert_classifier.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCHZ2Sdb6jbz"
      },
      "source": [
        "#### Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2bYSOj06kce"
      },
      "source": [
        "bert_classifier.fit(train_ds, train_label_ds, \n",
        "                    validation_data=(valid_ds, valid_label_ds), \n",
        "                    batch_size=GLOBAL_BATCH_SIZE,\n",
        "                    epochs=EPOCHS)\n",
        "\n",
        "# training for 8 epochs took approximately 1h 30m to run "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkRq6iJI8s5r"
      },
      "source": [
        "weights_dir = 'gs://PATH_TO_BUCKET/input_model/training_weights'\n",
        "bert_classifier.save_weights(weights_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG0rq2l39JZE"
      },
      "source": [
        "# Note: model should be reloaded and tested on GPU"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
